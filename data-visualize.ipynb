{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNssY8EK8hUmdIWmPTJ7cC0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxGvN2viP5UJ","executionInfo":{"status":"ok","timestamp":1716120250515,"user_tz":-540,"elapsed":26552,"user":{"displayName":"the korea","userId":"08516136404107173050"}},"outputId":"56d17586-6841-4ae1-eddd-88de3a4a9f30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/content/graph-wavenet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQH9GdglP8QM","executionInfo":{"status":"ok","timestamp":1716120250515,"user_tz":-540,"elapsed":4,"user":{"displayName":"the korea","userId":"08516136404107173050"}},"outputId":"638b8530-0a5a-42d6-952d-0da75d23ad1b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/content/graph-wavenet\n"]}]},{"cell_type":"markdown","source":["# Graph"],"metadata":{"id":"SAWAxMvovMDy"}},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","import os\n","import scipy.sparse as sp\n","import torch\n","from scipy.sparse import linalg\n","\n","\n","class DataLoader(object):\n","    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\n","        \"\"\"\n","        :param xs:\n","        :param ys:\n","        :param batch_size:\n","        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n","        \"\"\"\n","        self.batch_size = batch_size\n","        self.current_ind = 0\n","        if pad_with_last_sample:\n","            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n","            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n","            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n","            xs = np.concatenate([xs, x_padding], axis=0)\n","            ys = np.concatenate([ys, y_padding], axis=0)\n","        self.size = len(xs)\n","        self.num_batch = int(self.size // self.batch_size)\n","        self.xs = xs\n","        self.ys = ys\n","\n","    def shuffle(self):\n","        permutation = np.random.permutation(self.size)\n","        xs, ys = self.xs[permutation], self.ys[permutation]\n","        self.xs = xs\n","        self.ys = ys\n","\n","    def get_iterator(self):\n","        self.current_ind = 0\n","\n","        def _wrapper():\n","            while self.current_ind < self.num_batch:\n","                start_ind = self.batch_size * self.current_ind\n","                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n","                x_i = self.xs[start_ind: end_ind, ...]\n","                y_i = self.ys[start_ind: end_ind, ...]\n","                yield (x_i, y_i)\n","                self.current_ind += 1\n","\n","        return _wrapper()\n","\n","class StandardScaler():\n","    \"\"\"\n","    Standard the input\n","    \"\"\"\n","\n","    def __init__(self, mean, std):\n","        self.mean = mean\n","        self.std = std\n","\n","    def transform(self, data):\n","        return (data - self.mean) / self.std\n","\n","    def inverse_transform(self, data):\n","        return (data * self.std) + self.mean\n","\n","\n","\n","def sym_adj(adj):\n","    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n","    adj = sp.coo_matrix(adj)\n","    rowsum = np.array(adj.sum(1))\n","    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n","    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n","    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n","    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).astype(np.float32).todense()\n","\n","# def asym_adj(adj):\n","#     adj = sp.coo_matrix(adj)\n","#     rowsum = np.array(adj.sum(1)).flatten()\n","#     d_inv = np.power(rowsum, -1).flatten()\n","#     d_inv[np.isinf(d_inv)] = 0.\n","#     d_mat= sp.diags(d_inv)\n","#     return d_mat.dot(adj).astype(np.float32).todense()\n","\n","def calculate_normalized_laplacian(adj):\n","    \"\"\"\n","    # L = D^-1/2 (D-A) D^-1/2 = I - D^-1/2 A D^-1/2\n","    # D = diag(A 1)\n","    :param adj:\n","    :return:\n","    \"\"\"\n","    adj = sp.coo_matrix(adj)\n","    d = np.array(adj.sum(1))\n","    d_inv_sqrt = np.power(d, -0.5).flatten()\n","    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n","    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n","    normalized_laplacian = sp.eye(adj.shape[0]) - adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n","    return normalized_laplacian\n","\n","def calculate_scaled_laplacian(adj_mx, lambda_max=2, undirected=True):\n","    if undirected:\n","        adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n","    L = calculate_normalized_laplacian(adj_mx)\n","    if lambda_max is None:\n","        lambda_max, _ = linalg.eigsh(L, 1, which='LM')\n","        lambda_max = lambda_max[0]\n","    L = sp.csr_matrix(L)\n","    M, _ = L.shape\n","    I = sp.identity(M, format='csr', dtype=L.dtype)\n","    L = (2 / lambda_max * L) - I\n","    return L.astype(np.float32).todense()\n","\n","def load_pickle(pickle_file):\n","    try:\n","        with open(pickle_file, 'rb') as f:\n","            pickle_data = pickle.load(f)\n","    except UnicodeDecodeError as e:\n","        with open(pickle_file, 'rb') as f:\n","            pickle_data = pickle.load(f, encoding='latin1')\n","    except Exception as e:\n","        print('Unable to load data ', pickle_file, ':', e)\n","        raise\n","    return pickle_data\n","\n","def load_adj(pkl_filename, adjtype):\n","    sensor_ids, sensor_id_to_ind, adj_mx = load_pickle(pkl_filename)\n","    if adjtype == \"scalap\":\n","        adj = [calculate_scaled_laplacian(adj_mx)]\n","    elif adjtype == \"normlap\":\n","        adj = [calculate_normalized_laplacian(adj_mx).astype(np.float32).todense()]\n","    elif adjtype == \"symnadj\":\n","        adj = [sym_adj(adj_mx)]\n","    elif adjtype == \"transition\":\n","        adj = [asym_adj(adj_mx)]\n","    elif adjtype == \"doubletransition\":\n","        adj = [asym_adj(adj_mx), asym_adj(np.transpose(adj_mx))]\n","    elif adjtype == \"identity\":\n","        adj = [np.diag(np.ones(adj_mx.shape[0])).astype(np.float32)]\n","    else:\n","        error = 0\n","        assert error, \"adj type not defined\"\n","    return sensor_ids, sensor_id_to_ind, adj\n","\n","\n","def load_dataset(dataset_dir, batch_size, valid_batch_size= None, test_batch_size=None):\n","    data = {}\n","    for category in ['train', 'val', 'test']:\n","        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n","        data['x_' + category] = cat_data['x']\n","        data['y_' + category] = cat_data['y']\n","    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n","    # Data format\n","    for category in ['train', 'val', 'test']:\n","        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n","    data['train_loader'] = DataLoader(data['x_train'], data['y_train'], batch_size)\n","    data['val_loader'] = DataLoader(data['x_val'], data['y_val'], valid_batch_size)\n","    data['test_loader'] = DataLoader(data['x_test'], data['y_test'], test_batch_size)\n","    data['scaler'] = scaler\n","    return data\n","\n","def masked_mse(preds, labels, null_val=np.nan):\n","    if np.isnan(null_val):\n","        mask = ~torch.isnan(labels)\n","    else:\n","        mask = (labels!=null_val)\n","    mask = mask.float()\n","    mask /= torch.mean((mask))\n","    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n","    loss = (preds-labels)**2\n","    loss = loss * mask\n","    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n","    return torch.mean(loss)\n","\n","def masked_rmse(preds, labels, null_val=np.nan):\n","    return torch.sqrt(masked_mse(preds=preds, labels=labels, null_val=null_val))\n","\n","\n","def masked_mae(preds, labels, null_val=np.nan):\n","    if np.isnan(null_val):\n","        mask = ~torch.isnan(labels)\n","    else:\n","        mask = (labels!=null_val)\n","    mask = mask.float()\n","    mask /=  torch.mean((mask))\n","    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n","    loss = torch.abs(preds-labels)\n","    loss = loss * mask\n","    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n","    return torch.mean(loss)\n","\n","\n","def masked_mape(preds, labels, null_val=np.nan):\n","    if np.isnan(null_val):\n","        mask = ~torch.isnan(labels)\n","    else:\n","        mask = (labels!=null_val)\n","    mask = mask.float()\n","    mask /=  torch.mean((mask))\n","    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n","    loss = torch.abs(preds-labels)/labels\n","    loss = loss * mask\n","    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n","    return torch.mean(loss)\n","\n","\n","def metric(pred, real):\n","    mae = masked_mae(pred,real,0.0).item()\n","    mape = masked_mape(pred,real,0.0).item()\n","    rmse = masked_rmse(pred,real,0.0).item()\n","    return mae,mape,rmse"],"metadata":{"id":"ngXYCykxQAi8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def asym_adj(adj):\n","    adj = sp.coo_matrix(adj)\n","    print(adj)\n","    rowsum = np.array(adj.sum(1)).flatten()\n","    zero_rows_indices = np.where(rowsum == 0)[0]\n","    print(\"값이 0인 행의 인덱스:\", zero_rows_indices)\n","    d_inv = np.power(rowsum, -1).flatten()\n","    d_inv[np.isinf(d_inv)] = 0.\n","    d_mat= sp.diags(d_inv)\n","    return d_mat.dot(adj).astype(np.float32).todense()"],"metadata":{"id":"guTSMr-wSaZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sensor_ids, sensor_id_to_ind, adj_mx = load_adj(\"data/sensor_graph/adj_mx.pkl\", \"transition\") # doubletransition"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uCNffaHQeb1","executionInfo":{"status":"ok","timestamp":1715260184906,"user_tz":-540,"elapsed":2,"user":{"displayName":"the korea","userId":"08516136404107173050"}},"outputId":"bf6eaa79-320e-4252-ebdd-08434e8e59ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 197)\t0.84164745\n","  (2, 105)\t0.95196086\n","  (3, 5)\t0.67384326\n","  (3, 226)\t0.91460407\n","  (3, 242)\t0.61006176\n","  (4, 218)\t0.544323\n","  (4, 224)\t0.8655059\n","  (5, 3)\t0.67384326\n","  (8, 13)\t0.45311695\n","  (8, 247)\t0.9392061\n","  (9, 28)\t0.6288952\n","  (10, 43)\t0.4799595\n","  (12, 149)\t0.8552457\n","  (13, 8)\t0.45311695\n","  (13, 233)\t0.115326785\n","  (16, 163)\t0.2564609\n","  (19, 54)\t0.544323\n","  (19, 130)\t0.61948156\n","  (21, 23)\t0.7396096\n","  (22, 23)\t0.5480556\n","  (23, 21)\t0.7396096\n","  (23, 22)\t0.5480556\n","  (23, 25)\t0.11458396\n","  (25, 23)\t0.11458396\n","  (28, 9)\t0.6288952\n","  :\t:\n","  (225, 216)\t0.24736433\n","  (226, 3)\t0.91460407\n","  (226, 139)\t0.7660386\n","  (227, 146)\t0.93177927\n","  (227, 155)\t0.8698147\n","  (228, 147)\t0.759061\n","  (228, 148)\t0.83077157\n","  (229, 100)\t0.2878116\n","  (230, 113)\t0.9641302\n","  (230, 169)\t0.99268633\n","  (231, 186)\t0.9766294\n","  (231, 222)\t0.394516\n","  (233, 13)\t0.115326785\n","  (233, 66)\t0.61383015\n","  (234, 235)\t0.92167604\n","  (235, 156)\t0.91218835\n","  (235, 234)\t0.92167604\n","  (242, 3)\t0.61006176\n","  (242, 34)\t0.8837786\n","  (244, 245)\t0.9884793\n","  (245, 244)\t0.9884793\n","  (246, 107)\t0.5109651\n","  (247, 8)\t0.9392061\n","  (248, 192)\t0.22014414\n","  (248, 194)\t0.8669482\n","값이 0인 행의 인덱스: [  1   6   7  11  14  15  17  18  20  24  26  27  32  38  39  44  45  46\n","  48  49  50  51  52  53  58  69  70  97 104 106 109 110 117 119 120 121\n"," 123 124 125 131 134 136 137 151 152 153 154 166 185 198 201 202 206 207\n"," 213 217 232 236 237 238 239 240 241 243]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-52-a282301f96cb>:7: RuntimeWarning: divide by zero encountered in power\n","  d_inv = np.power(rowsum, -1).flatten()\n"]}]},{"cell_type":"markdown","source":["# 데이터셋"],"metadata":{"id":"W9m2ZzamvG9Z"}},{"cell_type":"code","source":["class DataLoader(object):\n","    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\n","        \"\"\"\n","        :param xs:\n","        :param ys:\n","        :param batch_size:\n","        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n","        \"\"\"\n","        self.batch_size = batch_size\n","        self.current_ind = 0\n","        if pad_with_last_sample:\n","            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n","            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n","            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n","            xs = np.concatenate([xs, x_padding], axis=0)\n","            ys = np.concatenate([ys, y_padding], axis=0)\n","        self.size = len(xs)\n","        self.num_batch = int(self.size // self.batch_size)\n","        self.xs = xs\n","        self.ys = ys\n","\n","    def shuffle(self):\n","        permutation = np.random.permutation(self.size)\n","        xs, ys = self.xs[permutation], self.ys[permutation]\n","        self.xs = xs\n","        self.ys = ys\n","\n","    def get_iterator(self):\n","        self.current_ind = 0\n","\n","        def _wrapper():\n","            while self.current_ind < self.num_batch:\n","                start_ind = self.batch_size * self.current_ind\n","                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n","                x_i = self.xs[start_ind: end_ind, ...]\n","                y_i = self.ys[start_ind: end_ind, ...]\n","                yield (x_i, y_i)\n","                self.current_ind += 1\n","\n","        return _wrapper()"],"metadata":{"id":"783YDV23wCMx","executionInfo":{"status":"ok","timestamp":1716120463864,"user_tz":-540,"elapsed":1,"user":{"displayName":"the korea","userId":"08516136404107173050"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class StandardScaler():\n","    \"\"\"\n","    Standard the input\n","    \"\"\"\n","\n","    def __init__(self, mean, std):\n","        self.mean = mean\n","        self.std = std\n","\n","    def transform(self, data):\n","        return (data - self.mean) / self.std\n","\n","    def inverse_transform(self, data):\n","        return (data * self.std) + self.mean"],"metadata":{"id":"HPk5MlBsv8lo","executionInfo":{"status":"ok","timestamp":1716120464243,"user_tz":-540,"elapsed":1,"user":{"displayName":"the korea","userId":"08516136404107173050"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def load_dataset(dataset_dir, batch_size, valid_batch_size= None, test_batch_size=None):\n","    data = {}\n","    for category in ['train', 'val', 'test']:\n","        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n","        data['x_' + category] = cat_data['x']\n","        data['y_' + category] = cat_data['y']\n","    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n","    # Data format\n","    for category in ['train', 'val', 'test']:\n","        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n","    data['train_loader'] = DataLoader(data['x_train'], data['y_train'], batch_size)\n","    data['val_loader'] = DataLoader(data['x_val'], data['y_val'], valid_batch_size)\n","    data['test_loader'] = DataLoader(data['x_test'], data['y_test'], test_batch_size)\n","    data['scaler'] = scaler\n","    return data"],"metadata":{"id":"9T3jVismRtxY","executionInfo":{"status":"ok","timestamp":1716120464243,"user_tz":-540,"elapsed":1,"user":{"displayName":"the korea","userId":"08516136404107173050"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import os\n","\n","dataset_dir = \"data/METR-LA\"\n","batch_size = 64\n","\n","dataloader = load_dataset(dataset_dir, batch_size, batch_size, batch_size)"],"metadata":{"id":"1hia9bxsvZrt","executionInfo":{"status":"ok","timestamp":1716120601312,"user_tz":-540,"elapsed":8495,"user":{"displayName":"the korea","userId":"08516136404107173050"}}},"execution_count":14,"outputs":[]}]}